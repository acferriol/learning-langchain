{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04d616b6",
   "metadata": {},
   "source": [
    "### Query Routing (Route the query to appropriate data source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f82762",
   "metadata": {},
   "source": [
    "### Logical Routing \n",
    "\n",
    "> We give the LLM knowledge of the various data sources at our disposal and then let the LLM reason which data source to apply based on the user’s query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24dcbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Data model Choose documentation\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    datasource: Literal[\"python_docs\", \"js_docs\"] = Field(\n",
    "        ...,\n",
    "        description=\"\"\"Given a user question, choose which datasource would be \n",
    "            most relevant for answering their question\"\"\",\n",
    "    )\n",
    "\n",
    "# LLM with function call \n",
    "llm = ChatOllama(model=\"qwen:0.5b\", temperature=0)\n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"You are an expert at routing a user question to the appropriate data \n",
    "    source.\n",
    "\n",
    "Based on the programming language the question is referring to, route it to the \n",
    "    relevant data source.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define router \n",
    "router = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3ff7391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python_docs'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"\"\"Why doesn't the following code work:\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
    "prompt.invoke(\"french\")\n",
    "\"\"\"\n",
    "\n",
    "result = router.invoke({\"question\": question})\n",
    "\n",
    "result.datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d33026e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'js_docs'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"\"\"Why doesn't the following code work:\n",
    "\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { z } from \"zod\";\n",
    "\n",
    "const llm = new ChatOpenAI({model: \"gpt-3.5-turbo\", temperature: 0})\n",
    "const structuredLlm = llm.withStructuredOutput(routeQuery, {name: \"RouteQuery\"})\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "result = router.invoke({\"question\": question})\n",
    "\n",
    "result.datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52add125",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Runnable \n",
    "def choose_route(result):\n",
    "    if \"python_docs\" in result.datasource.lower():\n",
    "        ### Logic here \n",
    "        return \"chain for python_docs\"\n",
    "    else:\n",
    "        ### Logic here \n",
    "        return \"chain for js_docs\"\n",
    "\n",
    "full_chain = router | choose_route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac4ece80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chain for js_docs'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dd01b9",
   "metadata": {},
   "source": [
    "### Semantic Routing\n",
    "\n",
    "> semantic routing involves embedding various prompts that represent various data sources alongside the user’s query and then performing vector similarity search to retrieve the most similar prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "922d5fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "# Two prompts\n",
    "physics_template = \"\"\"You are a very smart physics professor. You are great at \n",
    "    answering questions about physics in a concise and easy-to-understand manner. \n",
    "    When you don't know the answer to a question, you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering \n",
    "    math questions. You are so good because you are able to break down hard \n",
    "    problems into their component parts, answer the component parts, and then \n",
    "    put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d18d0dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed prompts\n",
    "embeddings = OllamaEmbeddings(model=\"qwen:0.5b\")\n",
    "prompt_templates = [physics_template, math_template]\n",
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1abe307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1024)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(prompt_embeddings).shape #Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17064d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16956028 0.16201   ]]\n",
      "A black hole is a region of space where the gravitational pull is so strong that nothing can escape from it. Black holes are incredibly dense and difficult to understand and study. However, there are many theories and models that attempt to explain the behavior of black holes.\n"
     ]
    }
   ],
   "source": [
    "# Route question to prompt\n",
    "@chain\n",
    "def prompt_router(query):\n",
    "    # Embed question\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    # Compute similarity\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)\n",
    "    print(similarity)\n",
    "    similarity = similarity[0]\n",
    "    # Pick the prompt most similar to the input question\n",
    "    most_similar = prompt_templates[similarity.argmax()]\n",
    "    return PromptTemplate.from_template(most_similar)\n",
    "\n",
    "semantic_router = (\n",
    "    prompt_router\n",
    "    | ChatOllama(model=\"qwen:0.5b\")\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(semantic_router.invoke(\"What's a black hole\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56354407",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
